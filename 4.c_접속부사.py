# -*- coding: utf-8 -*-
"""의자 긍정 속 부정 제거C 불용어제거

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1itrvlJ-aIKS-dbxwLAMBtWcHqqEQmPWF
"""


from konlpy.tag import Okt # komoran, hannanum, kkma, mecab / 항상같은것은 아니지만 konlpy안의 tag 파일을 참조하고 그중 Okt를 가져오겠다

import os

import numpy as np  # as 는 축약어
import pandas as pd

from datetime import datetime
import json
import re


from tqdm.notebook import tqdm

df = pd.read_csv('qqq.csv')

#행열전환
df = df.transpose()
df.rename(columns = df.iloc[0], inplace = True)
df= df.drop(df.index[0])

#행열전환 후 리스트화 
df_list = []
for i in range(len(df.columns)):
  lista = []
  for j in range(len(df)):
    
    if not pd.isna(df[df.columns[i]][j]):
      lista.append(df[df.columns[i]][j])
  df_list.append(lista)

#긍부정 키워드 사전 가져오기
aa = pd.read_table('긍정.txt', sep = '\n', header = None)

#긍부정 키워드 사전 리스트화
aaa = []
for i in  range(len(aa[0])):
  aaa.append(aa[0][i])

#접속부사 사전 가져오기
bb = pd.read_table('접속부사.txt', sep = '\n', header = None)

#접속부사 사전 리스트화
bbb = []
for i in  range(len(bb[0])):
  bbb.append(bb[0][i])

#원본 리뷰 중 접속부사 사전 키워드가 있는 단어의 좌표값 추출 
trash1 = []

for i in range(len(df_list)):
  for j in range(len(df_list[i])):
    if df_list[i][j].split(',')[0][2:-1] in bbb and j != 0:
      trash1.append([i,j])

#추출된 좌표값을 토대로 원본리뷰의 좌표값  추출
trash12 = []
for i in range(len(trash1)):
  trash12.append(trash1[i][0])

#접속부사 포함된 리뷰들만 따로 리스트화 >>>  이후 좌표값을 토대로 슬라이싱
df_list2 = []
for i in range(len(df_list)):
  for j in range(len(df_list[i])):

    if [i,j] in trash1:
      df_list2.append(df_list[i][0:j+1])
      df_list2.append(df_list[i][j+1:])

#접속부사가 포함되지 않은 리스트들과 접속부사 기준으로 슬라이싱 된 리스트 합치기
new_df_list = []

for i in range(len(df_list)):
  if i not in trash12:
    new_df_list.append(df_list[i])

for i in df_list2:
  new_df_list.append(i)

#긍부정 키워드를 이용하여 키워드가 포함되지 않은 리뷰들만 따로 리스트화
pure_p = []
pure_p_index= []
trash2 = []

for i in range(len(new_df_list)):
  for j in range(len(new_df_list[i])):
    if new_df_list[i][j].split(',')[0][2:-1] in aaa:
      trash2.append(i)
  
for i in range(len(new_df_list)):
  if i not in trash2:
    pure_p.append(new_df_list[i])
    pure_p_index.append(new_df_list[i])

#문장단위 리스트로 이루어진 순수 긍부정 리뷰를 단어단튀 리스트로 통합

df_p2 = []

for i in range(len(pure_p)):
  for j in range(len(pure_p[i])):
    df_p2.append([pure_p[i][j].split(',')[0][2:-1], pure_p[i][j].split(',')[1][2:-2]])

#특정 품사들만 골라내기

tag_sep = []

for i in range(len(df_p2)):
  pos = df_p2[i][1]
  if pos in ['Noun', 'Adjective' ]:
      tag_sep.append(df_p2[i][0])

#2글자 이상만 골라내기

df_p3 = []
for i in tag_sep:
  if len(i) >= 2:
    df_p3.append(i)

#불용어사전 가져오기

cc = pd.read_table('긍정 속 불용어.txt', sep = '\n', header = None)

#불용어사전 리스트화

ccc = []
for i in  range(len(cc[0])):
  ccc.append(cc[0][i])

#불용어사전에 포함된 키워드 제외하고 리스트 만들기

df_p4 = []
for i in range(len(df_p3)):
  if df_p3[i] not in ccc:
    df_p4.append(df_p3[i])

#데이터 프레임화
df = pd.DataFrame(df_p4)

#각 키워드의 개수 찾기
num_df = df[0].value_counts()

#엑셀 파일로 저장
num_df.to_excel(excel_writer='인기_긍정_순수_C.xlsx')

# pd.DataFrame(tag_sep).to_excel(excel_writer = 'sep2.xlsx')

# df2 = pd.DataFrame(pure_p_index)

# df2.to_excel(excel_writer='pure_p_C.xlsx')